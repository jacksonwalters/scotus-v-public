Ah, so I see what I was doing before. There is absolutely no need to install
tensorflow remotely. Unless there's a need to classify questions live, and
there isn't, you can and should just classify questions locally with a trained
model. What remains to be done is simply choose a model, and finish the script
which outputs the classified questions.

--------------------------------------------------------------------------------

Okay, so about three hours later, a lot was done! That was great.

So, after re-training a couple models, it became clear that the best way to go was to
use a CNN. Two reasons for that. First, even though there are more parameters,
it doesn't require any pre-trained wordvector weights, so it's a little simpler.
Second, it's able to pick out correlations among two and three word phrases a
bit better than alternatives. Finally, it's just more flexible, so that if you
have a lot more data, it would be the best architecture to use other than perhaps
an RNN. That's the model selection side.

Then some scripts were written to compress the notebook so that training can
happen easily via the command line. The model can then be created directly in
repo, where it is stored. I got rid of the placeholder model trained on IMDB
data.

I updated the script which loads the model. I set up some test examples which the
model successfully makes predictions on.

This is where we run into the issue of which data we're training the model with.
In order to make predictions, we need are vectorizing sentences using a
tokenizer which relies on the vocabulary from the tweet data, which the model is
trained on. This doesn't work for a different data set, such as the ANES political
opinion questions.

In order to alleviate this, the model can be generalized to accept text input,
in which case we would embed the vectorizer in the first layer. Alternatively,
the model can be trained on classified questions rather than tweets.

I prefer the second method. This means that it will be able to make predictions
for all questions, even if the predictions aren't very good. If all of the
questions are labeled (or most), then the model can be trained to memorize the
question data, which means it will essentially be as good as possible, or as good
as a human, at classifying the questions.

Further, one is free to concatenate the political tweet data, which would be an
interesting flavor.
